{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import networkx\n",
    "import requests_html\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "seen = set()\n",
    "driver = None\n",
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome(executable_path='/home/user/git/etudier/chromedriver')\n",
    "#global driver\n",
    "\n",
    "\n",
    "# write the output files\n",
    "#networkx.write_gexf(g, '%s.gexf' % args.output)\n",
    "#write_html(g, '%s.html' % args.output)\n",
    "\n",
    "# close the browser\n",
    "\n",
    "def get_cluster_id(url):\n",
    "    \"\"\"\n",
    "    Google assign a cluster identifier to a group of web documents\n",
    "    that appear to be the same publication in different places on the web.\n",
    "    How they do this is a bit of a mystery, but this identifier is\n",
    "    important since it uniquely identifies the publication.\n",
    "    \"\"\"\n",
    "    vals = parse_qs(urlparse(url).query).get('cluster', [])\n",
    "    if len(vals) == 1:\n",
    "        return vals[0]\n",
    "    else:\n",
    "        vals = parse_qs(urlparse(url).query).get('cites', [])\n",
    "        print(vals)\n",
    "        if len(vals) == 1:\n",
    "            return vals[0]\n",
    "    return None\n",
    "\n",
    "def get_id(e):\n",
    "    \"\"\"\n",
    "    Determining the publication id is tricky since it involves looking\n",
    "    in the element for the various places a cluster id can show up.\n",
    "    If it can't find one it will use the data-cid which should be\n",
    "    usable since it will be a dead end anyway: Scholar doesn't know of\n",
    "    anything that cites it.\n",
    "    \"\"\"\n",
    "    for a in e.find('.gs_fl a'):\n",
    "        if 'Cited by' in a.text:\n",
    "            return get_cluster_id(a.attrs['href'])\n",
    "        elif 'versions' in a.text:\n",
    "            return get_cluster_id(a.attrs['href'])\n",
    "    if 'data-cid' in e.attrs.keys():\n",
    "        return e.attrs['data-cid']\n",
    "    else:\n",
    "        print(e.attrs)\n",
    "def get_citations(url, depth=1, pages=1):\n",
    "    \"\"\"\n",
    "    Given a page of citations it will return bibliographic information\n",
    "    for the source, target of a citation.\n",
    "    \"\"\"\n",
    "    if url in seen:\n",
    "        return\n",
    "\n",
    "    html = get_html(url)\n",
    "    seen.add(url)\n",
    "\n",
    "    # get the publication that these citations reference.\n",
    "    # Note: this can be None when starting with generic search results\n",
    "    a = html.find('#gs_res_ccl_top a', first=True)\n",
    "    if a:\n",
    "        to_pub = {\n",
    "            'id': get_cluster_id(url),\n",
    "            'title': a.text,\n",
    "        }\n",
    "    else:\n",
    "        to_pub = None\n",
    "\n",
    "    for e in html.find('#gs_res_ccl_mid .gs_r'):\n",
    "\n",
    "        from_pub = get_metadata(e)\n",
    "        yield from_pub, to_pub\n",
    "\n",
    "        # depth first search if we need to go deeper\n",
    "        if depth > 0 and from_pub['cited_by_url']:\n",
    "            yield from get_citations(\n",
    "                from_pub['cited_by_url'],\n",
    "                depth=depth-1,\n",
    "                pages=pages\n",
    "            )\n",
    "\n",
    "    # get the next page if that's what they wanted\n",
    "    if pages > 1:\n",
    "        for link in html.find('#gs_n a'):\n",
    "            if link.text == 'Next':\n",
    "                yield from get_citations(\n",
    "                    'https://scholar.google.com' + link.attrs['href'],\n",
    "                    depth=depth,\n",
    "                    pages=pages-1\n",
    "                )\n",
    "\n",
    "def get_metadata(e):\n",
    "    \"\"\"\n",
    "    Fetch the citation metadata from a citation element on the page.\n",
    "    \"\"\"\n",
    "    article_id = get_id(e)\n",
    "\n",
    "    a = e.find('.gs_rt a', first=True)\n",
    "    if a:\n",
    "        url = a.attrs['href']\n",
    "        title = a.text\n",
    "    else:\n",
    "        url = None\n",
    "        title = e.find('.gs_rt .gs_ctu', first=True).text\n",
    "\n",
    "    authors = source = website = None\n",
    "    try:\n",
    "        meta = e.find('.gs_a', first=True).text\n",
    "    except:\n",
    "        print(e)\n",
    "    meta_parts = [m.strip() for m in re.split(r'\\W-\\W', meta)]\n",
    "    if len(meta_parts) == 3:\n",
    "        authors, source, website = meta_parts\n",
    "    elif len(meta_parts) == 2:\n",
    "        authors, source = meta_parts\n",
    "\n",
    "    if source and ',' in source:\n",
    "        year = source.split(',')[-1].strip()\n",
    "    else:\n",
    "        year = source\n",
    "\n",
    "    cited_by = cited_by_url = None\n",
    "    for a in e.find('.gs_fl a'):\n",
    "        if 'Cited by' in a.text:\n",
    "            cited_by = a.search('Cited by {:d}')[0]\n",
    "            cited_by_url = 'https://scholar.google.com' + a.attrs['href']\n",
    "\n",
    "    return {\n",
    "        'id': article_id,\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'year': year,\n",
    "        'cited_by': cited_by,\n",
    "        'cited_by_url': cited_by_url\n",
    "    }\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    get_html uses selenium to drive a browser to fetch a URL, and return a\n",
    "    requests_html.HTML object for it.\n",
    "\n",
    "    If there is a captcha challenge it will alert the user and wait until\n",
    "    it has been completed.\n",
    "    \"\"\"\n",
    "    global driver\n",
    "\n",
    "    time.sleep(random.randint(1,5))\n",
    "    driver.get(url)\n",
    "    while True:\n",
    "        try:\n",
    "            recap = driver.find_element_by_css_selector('#gs_captcha_ccl,#recaptcha')\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "\n",
    "            try:\n",
    "                html = driver.find_element_by_css_selector('#gs_top').get_attribute('innerHTML')\n",
    "                return requests_html.HTML(html=html)\n",
    "            except NoSuchElementException:\n",
    "                print(\"google has blocked this browser, reopening\")\n",
    "                driver.close()\n",
    "                driver = webdriver.Chrome()\n",
    "                return get_html(url)\n",
    "\n",
    "        print(\"... it's CAPTCHA time!\\a ...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "def remove_nones(d):\n",
    "    new_d = {}\n",
    "    for k, v in d.items():\n",
    "        if v is not None:\n",
    "            new_d[k] = v\n",
    "    return new_d\n",
    "\n",
    "def to_json(g):\n",
    "    j = {\"nodes\": [], \"links\": []}\n",
    "    for node_id, node_attrs in g.nodes(True):\n",
    "        node_attrs['id'] = node_id\n",
    "        j[\"nodes\"].append(node_attrs)\n",
    "    for source, target, attrs in g.edges(data=True):\n",
    "        j[\"links\"].append({\n",
    "            \"source\": source,\n",
    "            \"target\": target\n",
    "        })\n",
    "    return j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://scholar.google.com/scholar?hl=en&as_sdt=0%2C21&q=cscw+memory&btnG='\n",
    "depth=1\n",
    "pages=1\n",
    "# ready to start up headless browser\n",
    "#driver = webdriver.Chrome()\n",
    "\n",
    "# create our graph that will get populated\n",
    "g = networkx.DiGraph()\n",
    "\n",
    "# iterate through all the citation links\n",
    "results =  [(from_pub, to_pub) for from_pub, to_pub in get_citations(url, depth=depth, pages=pages)]\n",
    "    #if args.debug:\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (from_pub, to_pub) in results:\n",
    "    print('from: %s' % json.dumps(from_pub))\n",
    "    g.add_node(from_pub['id'], label=from_pub['title'], **remove_nones(from_pub))\n",
    "    if to_pub:\n",
    "        if args.debug:\n",
    "            print('to: %s' % json.dumps(to_pub))\n",
    "        print('%s -> %s' % (from_pub['id'], to_pub['id']))\n",
    "        g.add_node(to_pub['id'], label=to_pub['title'], **remove_nones(to_pub))\n",
    "        g.add_edge(from_pub['id'], to_pub['id'])\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
